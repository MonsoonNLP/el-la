{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpanishCounterfactuals.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e25cd042b0849ee8599a99994c7dd05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b0511af18b1f4ced891e971502764f16",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7f33cdffd5294b14b0d3cc4a0e85726e",
              "IPY_MODEL_a886f23e617145819374c9fae072ec36"
            ]
          }
        },
        "b0511af18b1f4ced891e971502764f16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f33cdffd5294b14b0d3cc4a0e85726e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_07415d2170694d7fb31818be2cdacd0c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 871891,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 871891,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3844fce147ee4fe0a9660bdd6ef969fd"
          }
        },
        "a886f23e617145819374c9fae072ec36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9c599524f3854d9ba2b6739b5fe15d6a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 872k/872k [00:01&lt;00:00, 825kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_415ab20baef74fdabfd516bd71045560"
          }
        },
        "07415d2170694d7fb31818be2cdacd0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3844fce147ee4fe0a9660bdd6ef969fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c599524f3854d9ba2b6739b5fe15d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "415ab20baef74fdabfd516bd71045560": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d23ac3865b38424cbc8ad5a7fc62f4da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a5858dfbb9b342ce8338a424e25348c8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e4d3a33d9a1645729e3c2d2e4da1df1c",
              "IPY_MODEL_04f1a2458f1b490a949123e8df6abd27"
            ]
          }
        },
        "a5858dfbb9b342ce8338a424e25348c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e4d3a33d9a1645729e3c2d2e4da1df1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0b9e8bc4d50d48f2a5efed9e3e7e2b53",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 625,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 625,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8619a39f517c460ebf3c8db3978e86e6"
          }
        },
        "04f1a2458f1b490a949123e8df6abd27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0b3108c12bf84a9cb5800e4a1063f2e6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 625/625 [00:28&lt;00:00, 22.1B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1defe64ca93a461e89dd246af7700b4b"
          }
        },
        "0b9e8bc4d50d48f2a5efed9e3e7e2b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8619a39f517c460ebf3c8db3978e86e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b3108c12bf84a9cb5800e4a1063f2e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1defe64ca93a461e89dd246af7700b4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dbddac59a106422b9bdba4b26d3e7a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6733513710034fcda907736a4f770441",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fa4d344f7c404370865dbac6d0a2da81",
              "IPY_MODEL_7feec931ab12437394bd5f73de31b8ae"
            ]
          }
        },
        "6733513710034fcda907736a4f770441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa4d344f7c404370865dbac6d0a2da81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_674794f3bccd426b9338412c71f3b932",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 672271273,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 672271273,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e68fe8e9d0d845739f6cdadcdfbd72ec"
          }
        },
        "7feec931ab12437394bd5f73de31b8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0bdf2ff93b254b829dfd6dd62402513b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 672M/672M [00:27&lt;00:00, 24.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6dd163f312d04be69aca332960b506c6"
          }
        },
        "674794f3bccd426b9338412c71f3b932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e68fe8e9d0d845739f6cdadcdfbd72ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0bdf2ff93b254b829dfd6dd62402513b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6dd163f312d04be69aca332960b506c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFSTeEAU8YC9",
        "colab_type": "text"
      },
      "source": [
        "# Spanish Counterfactuals\n",
        "## Background\n",
        "\n",
        "This notebook is the second part of a research project into gender bias in language models. Spanish has grammatical gender and different words for male and female professions (el doctor nuevo / la doctora nueva), so language models encode these words as separate points.  This is common in many languages, and doesn't necessarily introduce bias.\n",
        "\n",
        "**I chose Spanish only because I can understand and translate examples.**\n",
        "\n",
        "My goal is to adapt existing metrics for measuring bias, and propose more reusable solutions around gender bias in not-English NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo_zv6jk-2aH",
        "colab_type": "text"
      },
      "source": [
        "# Proposal\n",
        "\n",
        "Now it's time to talk solutions: I would like to evaluate models with an original sentence and a gender-flipped sentence. We can then test whether the outcome of any model is changed.\n",
        "\n",
        "**Can word embeddings be applied to flip gender in Spanish?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVKebIdH_r0K",
        "colab_type": "text"
      },
      "source": [
        "# Flipping individual words\n",
        "\n",
        "Let's load the BETO pretrained embeddings using HuggingFace's Transformers module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pdiVJOX9TSQ",
        "colab_type": "code",
        "outputId": "96495fae-45f5-4ca3-c62c-4ad97812502c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 40.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 32.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a750376f57b06080b181e6e337bda6093d15063d1de712e02f44bd42d08f0365\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F52d3FED_Lms",
        "colab_type": "code",
        "outputId": "79d083e6-3e38-4201-f677-883bbbc2fd30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import AutoTokenizer, BertModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
        "model = BertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG9PcwIL_xcy",
        "colab_type": "text"
      },
      "source": [
        "There are some methods to convert whole sentences to PyTorch tensors, but let's focus on individual words. Each word will come out of the language model as a 768-dimensional vector, which we can compare to each other "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx-WO-k3_xv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_for_word(word):\n",
        "  id = tokenizer.encode(word)[1]\n",
        "  return model.embeddings.word_embeddings.weight[id].detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx8PJE0__81p",
        "colab_type": "text"
      },
      "source": [
        "## Calculate vector differences between words\n",
        "\n",
        "Let's calculate ```article_diff``` between el and la"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0T-fwpL__kV",
        "colab_type": "code",
        "outputId": "969b1c98-b96b-41a1-9254-9402fbaf55b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# \"el\" minus \"la\"\n",
        "article_diff = embedding_for_word(\"el\") - embedding_for_word(\"la\")\n",
        "print(article_diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.01193652e-01 -9.56837833e-03  1.38982348e-02 -2.95504611e-02\n",
            " -4.62328494e-02 -3.54627706e-03 -3.15616056e-02  2.03622542e-02\n",
            " -3.60682607e-05  1.55523065e-02 -1.83134079e-02  1.56444181e-02\n",
            "  4.51911502e-02 -2.40496062e-02 -3.26022282e-02 -4.34471518e-02\n",
            "  3.63821909e-03 -3.73945720e-02 -1.38825476e-02  3.89223397e-02\n",
            "  1.16123660e-02 -1.03008505e-02 -3.97766568e-03 -3.00680920e-02\n",
            "  1.86911654e-02  7.11152926e-02  1.40250064e-02 -1.30725764e-02\n",
            "  2.60123964e-02 -1.65068321e-02  7.43721500e-02 -1.97625123e-02\n",
            "  5.68020567e-02 -1.81963909e-02  2.46390384e-02 -4.71784100e-02\n",
            " -1.56514253e-02 -3.83357853e-02  1.88288130e-02 -3.86340804e-02\n",
            "  5.18993661e-03  3.23170684e-02  1.57557055e-02  3.41297276e-02\n",
            "  2.10301094e-02  4.10606004e-02 -1.54591165e-04 -7.13625085e-03\n",
            " -7.02920230e-03 -5.48075140e-03 -4.54044901e-04 -1.09942835e-02\n",
            "  1.07189910e-02  2.06957944e-02  1.50508285e-02 -4.16118652e-04\n",
            "  5.66911185e-05  4.13249061e-02 -1.60544291e-02 -1.30373891e-02\n",
            "  1.46066807e-02 -5.94341755e-02  5.20033156e-03 -4.56437394e-02\n",
            " -2.61912588e-02  6.42203540e-03  1.75882280e-02 -3.25445831e-02\n",
            " -1.59308184e-02 -8.09715968e-03 -1.86514258e-02 -2.55161170e-02\n",
            " -3.72005254e-02 -4.61174101e-02  3.43833715e-02 -1.15720313e-02\n",
            " -1.51775610e-02 -2.42289379e-02  8.83357972e-03 -2.84404550e-02\n",
            " -3.43239121e-02 -4.34552580e-02 -2.47175060e-03 -8.78742523e-03\n",
            " -1.25778839e-03 -5.80074824e-03  2.69288570e-03  3.17259766e-02\n",
            "  2.02472713e-02 -4.99333665e-02  2.53687762e-02  6.97983801e-02\n",
            " -6.77060802e-03 -6.14191312e-03 -4.74820435e-02 -1.99057106e-02\n",
            " -3.21647339e-02  1.34251798e-02 -6.32137433e-03  2.57055257e-02\n",
            " -2.96395235e-02 -3.13004293e-02  2.73293313e-02  8.70183297e-03\n",
            "  9.32248309e-04 -1.93167254e-02  2.51928344e-03  8.88802782e-02\n",
            " -4.27684523e-02 -6.82077482e-02  4.11839783e-02  2.74796877e-02\n",
            "  2.91010011e-02 -1.62523687e-02  1.11278901e-02 -1.71412751e-02\n",
            "  2.44917236e-02 -1.24915428e-02  4.45778631e-02  4.41136323e-02\n",
            "  1.95585508e-02  1.87437329e-02  4.68398258e-03  5.27152047e-03\n",
            "  3.10648000e-03  1.11140683e-03 -2.03647800e-02  2.34365463e-02\n",
            " -3.01425941e-02 -1.99136371e-03  1.59646217e-02 -2.29459126e-02\n",
            "  2.35546939e-03  3.92686650e-02  4.26091440e-03  5.13182953e-02\n",
            " -8.96762777e-03  5.37602305e-02  3.13211605e-02 -2.97613814e-02\n",
            " -5.53865358e-03 -4.60429117e-03 -2.51329038e-02  2.24481747e-02\n",
            "  6.92113675e-03 -5.68447374e-02 -1.77443437e-02 -3.97901908e-02\n",
            " -1.92961264e-02  2.63657700e-02 -7.04304315e-04 -3.06830592e-02\n",
            " -7.07755834e-02  1.12126376e-02  1.10554053e-02  4.75630276e-02\n",
            "  1.72798801e-02 -1.76231787e-02  4.78545902e-03 -1.50985643e-02\n",
            " -2.24313736e-02  5.32974023e-03 -4.29076329e-02 -3.48566473e-03\n",
            "  1.03204288e-02  1.06208026e-03  1.35133937e-02 -4.41381037e-02\n",
            " -2.13790014e-02  3.64260003e-02 -1.76281016e-02  7.56203104e-03\n",
            " -3.72482315e-02 -2.23329868e-02  2.64877267e-03 -8.36483482e-03\n",
            "  5.33289835e-03  1.96398105e-02  1.34211257e-02 -9.53159947e-03\n",
            " -1.88368969e-02  2.12149620e-02 -8.23730696e-03 -7.04752281e-03\n",
            " -3.04818936e-02  5.59753217e-02 -9.37345624e-03 -3.23327035e-02\n",
            "  5.65689616e-03 -1.16545595e-02 -1.09751392e-02  4.61513512e-02\n",
            "  1.57336984e-02 -4.47180718e-02 -3.73838879e-02 -5.08305896e-03\n",
            "  1.88518651e-02  3.70478705e-02  2.72784219e-03 -1.73056070e-02\n",
            "  1.50983408e-03  3.33863986e-03 -3.52853872e-02  2.95477845e-02\n",
            "  1.79019831e-02  3.13048344e-03 -1.22252051e-02  1.55245783e-02\n",
            " -1.34881437e-02  1.79194495e-01 -8.86581540e-02 -2.14745700e-02\n",
            " -2.18991004e-02 -1.41820181e-02 -1.16051268e-02  4.12911437e-02\n",
            "  1.98272318e-02  1.08722597e-03 -2.25424375e-02  5.22297248e-02\n",
            " -7.64313899e-03  4.09124093e-03  4.50113714e-02  9.14728642e-03\n",
            " -6.63536415e-03 -1.96506456e-02  1.13002099e-02  1.26992855e-02\n",
            "  5.24229370e-02 -1.41786039e-02  5.61861247e-02 -3.37155350e-02\n",
            " -2.34386884e-03 -3.88903879e-02  4.10501957e-02  1.65739786e-02\n",
            " -1.78189538e-02  1.21767065e-02 -2.05102935e-03  6.91635348e-03\n",
            "  6.04577363e-04 -6.99473359e-03 -1.92901976e-02  2.18706708e-02\n",
            " -3.86071131e-02 -9.69772935e-02  4.59806621e-03 -3.53083089e-02\n",
            "  4.46364423e-03  2.86614113e-02  2.40385570e-02 -4.07306254e-02\n",
            "  2.06989311e-02  6.38760030e-02 -1.09270420e-02 -2.31329240e-02\n",
            " -4.43978757e-02 -2.58777887e-02 -1.01130009e-02  1.06862132e-02\n",
            " -4.17405888e-02  5.53550944e-03 -1.21689942e-02 -1.35115879e-02\n",
            " -4.75993603e-02 -6.36405312e-04 -8.88127647e-03 -1.28909014e-03\n",
            " -1.04208244e-02 -2.77272183e-02  5.46627119e-03 -1.42216776e-02\n",
            "  4.14646342e-02 -5.52503280e-02  3.04840617e-02 -4.91875187e-02\n",
            "  5.41967060e-03 -9.51463357e-03 -1.27767110e-02 -2.98716761e-02\n",
            "  2.81942561e-02  2.94575207e-02  3.29030007e-02  3.13119851e-02\n",
            " -9.76631418e-04  1.26893930e-02 -2.37862254e-03 -9.55087692e-03\n",
            "  2.18049008e-02 -3.06226350e-02  2.02408880e-02  5.75490445e-02\n",
            "  1.32229179e-02 -1.41971475e-02  2.86945030e-02 -2.22993735e-02\n",
            "  1.85855776e-02  9.44998581e-03 -3.27132046e-02  3.14668342e-02\n",
            " -2.27301382e-02 -5.39513230e-02  1.79930218e-02 -9.71671008e-03\n",
            "  1.33492472e-02  1.77824646e-02  3.06912735e-02  2.44564470e-03\n",
            "  4.38109459e-03  4.87180650e-02 -1.47670507e-03  6.65822439e-03\n",
            " -1.26028910e-01 -1.81782935e-02 -1.64046176e-02  4.73943353e-02\n",
            " -1.06542334e-02  2.33915895e-02 -1.25571359e-02  2.32405309e-03\n",
            "  4.00332473e-02  1.46958753e-02  1.13334283e-01  1.63490679e-02\n",
            "  2.16440111e-03 -8.95152241e-03 -4.43013292e-03  1.97407901e-02\n",
            "  4.90892231e-02 -3.03480439e-02  8.41476023e-03 -6.47852570e-03\n",
            "  1.02560017e-02 -8.69906507e-03 -9.97222960e-03  2.37516612e-02\n",
            "  2.39682291e-02  1.91316381e-02 -1.96228437e-02 -3.96723151e-02\n",
            " -1.61798205e-02 -1.87834576e-02 -4.86348085e-02  1.89700648e-02\n",
            "  4.93062176e-02 -5.37783466e-03  2.80264858e-02  2.73477435e-02\n",
            " -4.49787974e-02  2.84687411e-02  1.08773746e-02  9.68024507e-03\n",
            " -1.03092659e-02 -3.59185189e-02 -2.09379233e-02  7.58690387e-03\n",
            " -3.06093134e-02 -3.81571241e-02  6.31642267e-02 -1.05948495e-02\n",
            "  1.50742605e-02 -1.48276724e-02 -1.55553371e-02  1.23295505e-02\n",
            "  2.83201551e-03  3.58782560e-02 -4.79241461e-03  3.29849590e-03\n",
            " -1.12481266e-02 -1.13182366e-02 -1.48302261e-02  1.77663732e-02\n",
            " -9.75697488e-03  2.62618773e-02 -5.37141636e-02 -1.83493830e-02\n",
            "  2.32324768e-02  1.09399948e-03 -5.35637401e-02  5.18713472e-03\n",
            " -2.03325655e-02  7.14520831e-03  3.23241055e-02  4.39071096e-03\n",
            " -1.04902238e-02 -4.30351943e-02  1.46635948e-02 -4.04638052e-03\n",
            " -1.76974200e-02  2.39015482e-02 -3.37127037e-02 -1.54311694e-02\n",
            " -2.80039608e-02 -5.73519915e-02 -2.32245121e-03  1.32729411e-02\n",
            " -1.72769371e-03  1.35363296e-01  5.33149131e-02 -2.47716904e-02\n",
            "  3.21234539e-02  3.18323858e-02 -7.50571210e-03  8.78945924e-03\n",
            " -7.82458112e-03  9.45430994e-03 -8.31089541e-03  1.05063589e-02\n",
            " -3.84188220e-02  2.09171493e-02 -8.42488464e-03  2.21130550e-02\n",
            "  1.07508367e-02 -2.59837937e-02 -4.71632667e-02  2.84901746e-02\n",
            " -2.58174967e-02  3.29790264e-02 -2.31012516e-02  1.08652562e-03\n",
            " -3.48093249e-02 -8.43541697e-03  2.50767749e-02 -6.41657114e-02\n",
            "  3.42634879e-03 -9.10254195e-03  1.44528467e-02  4.61002588e-02\n",
            " -1.57214887e-02  2.40779147e-02 -1.51890647e-02  3.85749899e-02\n",
            "  2.13841237e-02 -1.88891683e-02  1.76603198e-02  2.73170918e-02\n",
            " -9.44176875e-03  6.00259285e-03  6.82333065e-03 -2.89037824e-02\n",
            " -3.61981988e-02 -4.70972136e-02  3.11587229e-02  3.74291986e-02\n",
            " -3.56304795e-02 -4.21119258e-02  6.31415397e-02 -1.30201168e-02\n",
            "  7.31776562e-03 -1.56879015e-02 -3.91398184e-03  2.07175482e-02\n",
            " -3.52672078e-02  3.30932252e-03 -1.34909283e-02 -1.78081635e-02\n",
            " -3.76380496e-02  1.89536512e-02  2.02280879e-02 -9.60797071e-03\n",
            "  6.06062636e-03 -4.64854501e-02 -2.00694334e-03 -7.86888786e-03\n",
            "  5.01373224e-02  2.60496903e-02 -1.22306421e-02  2.05199756e-02\n",
            " -1.93272159e-02  6.75045233e-03 -2.36557908e-02 -1.23108178e-03\n",
            "  3.91675383e-02 -2.33855229e-02 -4.75146249e-03  3.77938570e-03\n",
            " -2.31606141e-03  5.64640015e-03  4.89049964e-03  4.42022718e-02\n",
            "  1.07275099e-02 -5.14334813e-02 -7.12346693e-04 -2.30825264e-02\n",
            "  9.07750335e-03  7.93195888e-03 -1.31940115e-02 -6.59545325e-03\n",
            "  1.31244771e-02 -6.19130805e-02  3.48897278e-03  1.28712039e-03\n",
            "  2.31628120e-02  2.70798281e-02 -1.87387634e-02  4.69712429e-02\n",
            " -6.33648969e-03  1.06058083e-02 -4.22030836e-02  1.95468217e-03\n",
            "  6.71184249e-03 -4.47657704e-03  7.63964094e-03  1.24034509e-02\n",
            " -3.01378258e-02  8.70514754e-03 -2.38068011e-02  5.88531606e-04\n",
            "  1.63151305e-02 -3.47651690e-02 -4.31322027e-03  3.51940244e-02\n",
            " -1.04724877e-02  1.64591800e-02  3.74003053e-02 -2.22581252e-02\n",
            " -2.37537455e-02  7.21512269e-03 -1.48929898e-02 -2.18750052e-02\n",
            "  6.40632957e-02  3.35294493e-02  2.76272725e-02 -9.98581760e-03\n",
            " -1.99691765e-03 -6.80244528e-03 -6.90608658e-03 -1.52385272e-02\n",
            "  2.95423418e-02  1.31085403e-02  1.12793660e-02 -9.01155826e-03\n",
            "  7.56497588e-03 -1.51949283e-02  6.29790500e-03 -3.62081826e-02\n",
            " -6.20079041e-03 -1.48247918e-02  9.88479238e-03 -3.61730903e-02\n",
            "  6.19414635e-03  5.72345033e-02  5.06016798e-03 -2.54522394e-02\n",
            "  7.42691755e-03  1.69251077e-02  8.98935832e-04 -2.46405881e-03\n",
            " -5.77043407e-02 -4.70468178e-02  1.19823813e-02  1.77699979e-02\n",
            " -1.35935312e-02  1.81803592e-02  4.93733026e-03  1.37529476e-03\n",
            "  3.35249305e-02 -4.48768288e-02 -5.76272607e-03 -1.71822570e-02\n",
            "  3.13753262e-03 -1.93664916e-02  3.77484746e-02  3.81231494e-03\n",
            "  1.03846155e-02  3.71295139e-02  1.08010797e-02  2.00522207e-02\n",
            "  5.03570326e-02 -6.12417310e-02  2.65527815e-02 -2.27340385e-02\n",
            " -3.62570733e-02 -8.87997448e-05  7.32718408e-02  1.66669134e-02\n",
            " -1.23866349e-02 -1.01088099e-02  1.42805558e-03 -3.70833576e-02\n",
            "  3.14719826e-02  1.32842977e-02  2.57453024e-02  1.11727463e-03\n",
            " -6.69721700e-03 -2.20294446e-02 -3.19866650e-03 -9.84480954e-04\n",
            "  1.07567534e-02 -1.56509392e-02 -3.20115350e-02 -2.14685798e-02\n",
            " -5.42745814e-02 -2.55722739e-02 -3.82949598e-02 -1.26486802e-02\n",
            "  1.63692348e-02  3.42233554e-02 -6.68865209e-03 -7.48512149e-03\n",
            " -7.04101985e-03 -1.47030801e-02 -1.17491260e-02 -3.05409152e-02\n",
            " -2.64435783e-02 -1.42560741e-02 -5.20731285e-02 -9.82651487e-03\n",
            " -2.97613833e-02 -8.12213868e-03 -9.00630746e-03 -2.72366386e-02\n",
            "  1.06863901e-02 -1.10499673e-02  1.10440683e-02 -1.01347212e-02\n",
            " -2.16072574e-02  2.09666602e-02  1.18037872e-03 -1.08701438e-02\n",
            "  6.80601597e-03  9.43350513e-03 -4.10909802e-02  3.34172100e-02\n",
            "  3.57436277e-02  1.32684615e-02 -8.73675384e-03 -1.97019447e-02\n",
            " -3.06422934e-02 -2.09329762e-02  3.11059430e-02 -4.87705320e-03\n",
            " -5.53851947e-04 -1.15529366e-01  8.88389535e-03 -5.88089228e-04\n",
            "  1.10905860e-02 -1.65932830e-02 -1.68016702e-02 -1.85601953e-02\n",
            "  2.12454069e-02 -8.31151847e-03  4.17453647e-02  1.23705640e-02\n",
            "  1.48134455e-02 -8.43546987e-02  2.75197960e-02 -2.85889711e-02\n",
            "  2.19819918e-02 -1.50445309e-02  1.49082979e-02  4.01635468e-03\n",
            "  8.81484803e-03  3.53335366e-02 -2.20190734e-03 -2.75152195e-02\n",
            " -5.80010563e-03  1.27036590e-02  2.17763055e-03  3.69881466e-03\n",
            " -1.68199036e-02 -3.05675343e-02  2.75271237e-02 -3.73383984e-03\n",
            "  3.36962938e-02 -5.20520806e-02  1.51509643e-02  1.51018929e-02\n",
            "  8.94093886e-04 -5.71727939e-03 -2.95433514e-02  2.77311224e-02\n",
            " -4.31423485e-02 -4.10452485e-04 -1.60266273e-02 -8.72086175e-03\n",
            "  1.42191257e-03 -6.12053461e-03 -2.78402325e-02 -1.19588207e-02\n",
            " -3.33250277e-02  8.58448166e-03 -2.77552791e-02  3.81470099e-02\n",
            "  5.30542154e-03 -1.81870759e-02 -6.09596148e-02 -7.49620274e-02\n",
            " -9.83723160e-03 -3.45194228e-02 -7.63017833e-02  2.08268240e-02\n",
            " -7.06701428e-02 -5.72368838e-02 -1.90739315e-02 -2.57506967e-02\n",
            "  3.23837288e-02 -9.57583450e-03 -1.36423521e-02 -1.90606378e-02\n",
            " -2.09619999e-02 -3.47094387e-02  3.48436981e-02 -1.34259090e-03\n",
            "  3.19479145e-02  3.99588868e-02 -2.48506721e-02 -7.80250132e-03\n",
            " -3.75517681e-02 -4.81843529e-03  5.87425753e-03  5.01246527e-02\n",
            "  9.76611860e-03 -1.17049739e-02 -2.28521898e-02 -2.37093642e-02\n",
            "  1.40807023e-02  1.81968324e-02  9.65745188e-03 -3.78411915e-03\n",
            "  2.88343243e-02 -2.17593592e-02  4.96864170e-02  3.88380364e-02\n",
            "  1.13070700e-02 -1.64658483e-02 -2.12097540e-03  1.31765902e-02\n",
            "  1.67836417e-02  4.25054505e-02  4.26963344e-02 -2.59486977e-02\n",
            " -5.17396182e-02 -2.78700162e-02  8.53954162e-03  1.41015481e-02\n",
            "  2.29735207e-02  2.75656674e-02 -4.51579876e-02 -7.24156573e-03\n",
            "  1.37429573e-02  2.25507040e-02 -1.36035243e-02 -2.24405080e-02\n",
            " -1.97578240e-02  3.46079655e-02  2.45334934e-02 -1.30252168e-02\n",
            " -3.28315347e-02 -4.28749658e-02 -6.71842396e-02 -1.12341819e-02\n",
            "  3.17387730e-02 -3.01723443e-02 -2.91693062e-02 -5.27437106e-02\n",
            " -3.22658718e-02 -2.10231282e-02  2.38509849e-04  5.49296848e-03\n",
            "  3.66584770e-02 -5.10344692e-02  3.26640792e-02 -3.29413489e-02\n",
            " -5.48995659e-03 -2.52084918e-02 -1.67125184e-02 -4.83771786e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSnoH1qf_fUI",
        "colab_type": "text"
      },
      "source": [
        "Let's calculate a ```noun_diff``` vector between a male and female profession (such as maestro/maestra)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZonb3YhAD1M",
        "colab_type": "code",
        "outputId": "3c57bae8-cb95-44ef-8d42-58ed071bde79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# \"maestro\" - \"maestra\"\n",
        "noun_diff = embedding_for_word(\"maestro\") - embedding_for_word(\"maestra\")\n",
        "print(noun_diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-3.59191820e-02 -9.68925003e-03  1.42796785e-02 -9.90152545e-03\n",
            "  6.92520216e-02 -3.00704166e-02 -6.19686171e-02  1.14616267e-02\n",
            " -1.72810331e-02  1.39672980e-02  3.81249674e-02  1.35235228e-02\n",
            " -3.24731506e-02  3.27836163e-03 -9.65690147e-03 -3.12734097e-02\n",
            "  3.68267298e-02  3.58559191e-02  8.01959634e-03  3.57991792e-02\n",
            "  8.31423178e-02 -5.37066087e-02  1.66588556e-02  9.48078930e-03\n",
            "  2.42801681e-02  4.36856858e-02  3.07860076e-02  5.66809773e-02\n",
            " -2.23038439e-02  1.80876181e-02  1.41317472e-02  5.07193059e-03\n",
            "  1.09874308e-02 -1.31353280e-02  3.02872658e-02  2.42619384e-02\n",
            " -1.82333421e-02  7.79369920e-02 -2.75658667e-02 -4.51937839e-02\n",
            "  4.20431532e-02 -1.06230872e-02  1.15213096e-01 -3.30538377e-02\n",
            "  5.36187887e-02  4.96502407e-03  2.27976888e-02 -4.73800227e-02\n",
            "  3.71647961e-02  6.89722896e-02 -2.12735981e-02 -3.60166579e-02\n",
            " -3.63069773e-03  3.16753313e-02 -8.59868713e-03  4.13877517e-03\n",
            "  1.91910006e-02  6.44873828e-03  4.26523313e-02  5.76551221e-02\n",
            "  9.25509725e-03  1.97832435e-02 -2.83869039e-02 -8.50492343e-02\n",
            " -4.53510061e-02  1.27418898e-02  1.15483943e-02 -4.72691655e-03\n",
            " -5.41209802e-03  5.75828031e-02 -8.28262419e-05 -3.28807347e-02\n",
            " -3.13897692e-02 -9.23676416e-04  7.03086406e-02  3.01419944e-02\n",
            "  6.60277605e-02  3.69034410e-02 -1.91766992e-02  7.25265369e-02\n",
            "  3.57974991e-02 -6.51638657e-02  2.23780293e-02  3.11572757e-02\n",
            "  9.98089761e-02 -2.87068691e-02  5.77881793e-03  9.07727238e-03\n",
            " -1.43514760e-02 -1.80970058e-02 -5.37334755e-02 -2.30529457e-02\n",
            "  1.78606901e-02  5.43542765e-02  4.49751504e-03 -2.08101291e-02\n",
            " -1.90835074e-03  5.48015535e-03 -1.69427097e-02 -4.91047800e-02\n",
            "  1.38224140e-02  4.93097566e-02  3.30162868e-02 -4.93833609e-03\n",
            " -3.76682915e-03 -2.52623875e-02  6.38459250e-03  2.76417248e-02\n",
            "  3.59355882e-02 -2.42076963e-02 -2.58584619e-02  1.59184709e-02\n",
            " -3.88211347e-02 -1.04278997e-02 -1.37737021e-03  4.47064079e-03\n",
            "  5.55630587e-03 -2.31807977e-02 -2.66040601e-02 -1.45338932e-02\n",
            "  7.05048442e-03  3.47813666e-02  1.26542971e-02  2.98164487e-02\n",
            " -6.04116172e-03  2.56915465e-02 -2.11599469e-02 -2.51892745e-03\n",
            " -3.45393457e-02  1.19893365e-02 -1.36760771e-02 -4.88149375e-02\n",
            "  6.32100552e-02 -5.04941866e-02  3.08864024e-02 -5.04430234e-02\n",
            "  2.50428114e-02 -3.22277695e-02  4.13687900e-02  3.37800682e-02\n",
            "  4.96559888e-02 -8.75680298e-02  3.84765901e-02  7.02938735e-02\n",
            "  3.34811993e-02  6.79942966e-02  6.02388084e-02 -3.80245224e-02\n",
            "  1.19588189e-02  6.52151480e-02 -5.63885737e-03  2.54435837e-02\n",
            "  7.65500963e-03 -7.78482109e-03 -7.13600293e-02  3.89596224e-02\n",
            "  9.20938049e-03  3.97469401e-02 -2.97624208e-02 -4.31456454e-02\n",
            " -1.70162879e-02  1.69935673e-02  2.48550735e-02  1.89864989e-02\n",
            " -5.88905439e-02  1.80884078e-03 -2.63051987e-02 -7.71408714e-03\n",
            "  6.01999909e-02 -1.44627760e-03  8.70213192e-03  7.47269541e-02\n",
            "  4.41854559e-02 -1.17750354e-02 -6.49031699e-02 -1.12405997e-02\n",
            " -5.16223442e-03  3.36405411e-02  1.26981437e-02 -1.67077407e-02\n",
            "  2.89718248e-03 -3.65100801e-03  4.99850661e-02 -1.70203838e-02\n",
            "  5.87155521e-02  2.75796670e-02 -3.21164280e-02 -6.08275011e-02\n",
            " -7.00600743e-02  1.55445039e-02  2.08315793e-02  5.55292517e-02\n",
            "  3.67707610e-02 -4.37405705e-02 -9.23636742e-03 -7.39006475e-02\n",
            " -1.26533136e-02 -2.31659412e-02 -1.02258809e-02  2.67312042e-02\n",
            "  4.82459068e-02 -4.14870307e-03 -7.99988024e-03 -1.45455636e-03\n",
            "  3.27728838e-02 -4.25091758e-02 -3.34391184e-03  8.58508721e-02\n",
            "  1.53944455e-02 -8.10880214e-04  3.84294316e-02 -2.96169817e-02\n",
            " -1.31788161e-02  5.90838939e-02  1.58422515e-02  5.03266603e-03\n",
            "  4.03542072e-04 -1.62392035e-02  1.97074413e-02  4.61894870e-02\n",
            " -2.52186153e-02  8.96468200e-03  7.15283751e-02 -3.25932615e-02\n",
            "  3.96526381e-02  6.23711571e-03 -4.53336686e-02  1.60844848e-02\n",
            "  1.05000930e-02 -6.24142587e-04  4.95959409e-02 -3.66018228e-02\n",
            "  3.69725823e-02  1.84322856e-02 -4.45585884e-02 -5.98202199e-02\n",
            "  1.71707217e-02 -3.30495834e-02  2.83237845e-02 -1.95318293e-02\n",
            " -8.17677677e-02  1.23466067e-02  9.20064747e-03 -5.72580025e-02\n",
            "  4.92322706e-02 -3.31186168e-02 -1.99674033e-02  4.29047644e-02\n",
            " -1.85894705e-02  1.03020146e-02  3.05316178e-03 -3.13785300e-02\n",
            "  2.17803344e-02  8.68380815e-02 -6.88342676e-02 -6.75714239e-02\n",
            "  1.09648705e-02  8.65178853e-02 -4.57015634e-03 -4.48842254e-03\n",
            " -5.34976348e-02 -6.18362054e-02 -6.28843829e-02 -3.26388776e-02\n",
            " -5.82740866e-02  1.15793198e-03 -1.59459189e-02  3.58350538e-02\n",
            " -7.48203397e-02 -1.81333534e-03  3.91121767e-02  5.06456271e-02\n",
            " -4.99147177e-02 -4.83424962e-03  3.78376618e-03 -5.10441512e-03\n",
            " -6.17750287e-02  1.25311632e-02  4.69834283e-02 -6.42345697e-02\n",
            "  2.91796699e-02  2.05338690e-02 -2.00224202e-02  7.26576149e-02\n",
            "  6.49663508e-02  2.09830049e-02  2.34336909e-02  4.46625948e-02\n",
            "  6.12935312e-02  1.69487298e-03  6.91503584e-02 -1.76374149e-02\n",
            " -2.66977935e-03 -5.62815368e-03 -2.10782513e-02  9.84792225e-03\n",
            "  1.87078752e-02 -5.59333973e-02  1.31256906e-02 -3.96338627e-02\n",
            "  3.22010703e-02 -5.38451038e-02  5.15003316e-02 -7.96748772e-02\n",
            "  1.01688683e-01  9.88599658e-03 -7.90947750e-02  2.78093554e-02\n",
            " -1.86277777e-02  2.82316729e-02 -1.56494752e-02 -9.20182094e-03\n",
            "  8.18571076e-03  6.09388612e-02 -2.71708034e-02  3.50672007e-03\n",
            " -7.37297162e-03  1.73078328e-02 -6.52946606e-02  4.81915921e-02\n",
            "  8.56511295e-04 -5.46064600e-03 -5.74667789e-02 -6.33802116e-02\n",
            "  1.78039335e-02 -3.31163779e-03 -2.12886333e-02  9.13076773e-02\n",
            "  1.22953787e-01 -1.18174404e-02  4.04050499e-02  6.60477728e-02\n",
            "  4.82112840e-02  5.21611087e-02 -6.71840385e-02  5.08595258e-03\n",
            " -4.50790823e-02  1.85616575e-02  7.44904056e-02 -1.86605696e-02\n",
            " -1.49118993e-02 -3.78035195e-02 -4.51557860e-02  3.00393105e-02\n",
            "  2.23676208e-02 -6.34412840e-03 -9.30304453e-03 -4.39081863e-02\n",
            " -4.08515334e-05  2.89602205e-03 -4.09213528e-02  1.57630350e-02\n",
            " -2.32507866e-02  5.08361273e-02  5.94791025e-02  3.78595106e-03\n",
            "  2.30579916e-03 -7.92285241e-03 -2.79696546e-02 -7.99108483e-03\n",
            "  5.50753921e-02  9.10132974e-02  6.87335134e-02  3.29783112e-02\n",
            "  9.80958343e-03  1.11478753e-02 -4.68753539e-02  4.29975837e-02\n",
            "  1.13646835e-02  1.63931213e-03  6.24768157e-03  1.43273398e-02\n",
            " -1.49900876e-02  6.01423196e-02  2.41177008e-02  4.32006381e-02\n",
            " -4.89327312e-03 -1.70844346e-02  5.52178100e-02  2.34999806e-02\n",
            " -8.88467208e-03 -8.76164064e-03  5.99543750e-03  5.04884683e-02\n",
            "  7.61640444e-03 -3.54402252e-02 -2.91175842e-02  5.60247414e-02\n",
            " -2.50687879e-02 -5.39572909e-02 -5.17386664e-03 -2.01527253e-02\n",
            "  4.07138579e-02 -6.06127381e-02 -2.14836486e-02  1.33738875e-01\n",
            " -1.25843789e-02  5.81815392e-02 -6.09822385e-03  3.00509110e-03\n",
            "  1.94996670e-02 -5.28488569e-02  1.38887644e-01 -2.09219847e-03\n",
            " -5.12590632e-03 -1.94357559e-02  4.27347161e-02  3.05165537e-04\n",
            " -3.56308855e-02 -3.93970683e-03 -3.41731124e-02 -4.50785831e-02\n",
            "  4.06466536e-02 -6.14360068e-03  1.33352242e-02 -9.22063664e-02\n",
            "  3.72790396e-02 -5.95740974e-02 -3.49398330e-03  1.18112341e-02\n",
            "  6.77726232e-03  7.22829551e-02  2.69264188e-02  3.58304083e-02\n",
            " -7.91693851e-03 -2.18856558e-02  2.77472790e-02 -6.19111881e-02\n",
            " -2.34747194e-02  2.76404601e-02  3.28153186e-02  2.11719349e-02\n",
            "  6.31478801e-02  1.45062916e-02 -5.96563518e-03 -2.10411847e-02\n",
            " -1.21912528e-02  3.39559093e-03 -2.96912380e-02 -3.39428857e-02\n",
            " -2.79337354e-02  5.01079410e-02  1.33182779e-02 -3.41552831e-02\n",
            "  1.11849979e-02 -7.63421133e-03  4.00543585e-02  4.71255481e-02\n",
            " -6.63263872e-02  4.83929813e-02  4.05499861e-02  3.31930816e-03\n",
            "  3.84980142e-02 -8.89549851e-02  1.70514919e-04 -6.35317266e-02\n",
            "  6.30792677e-02  4.53176945e-02  6.60663620e-02 -7.12111406e-03\n",
            " -5.86481355e-02  4.36553732e-02  3.09944358e-02 -5.25205061e-02\n",
            " -1.27493031e-02 -6.15900122e-02 -4.51078489e-02  2.52523478e-02\n",
            "  4.68766987e-02  4.70228568e-02 -7.64189288e-03 -6.07089996e-02\n",
            "  2.37336056e-03  1.46063417e-03 -5.20727299e-02  1.29586291e-02\n",
            " -2.78512761e-02 -2.78826673e-02  5.47891408e-02  3.18013877e-02\n",
            "  9.17270128e-03 -2.26511266e-02  1.67656094e-02  5.02507202e-02\n",
            " -5.12599945e-02  3.69713269e-03 -3.71161215e-02  7.16564991e-03\n",
            " -5.33574633e-02  3.64782959e-02 -3.75045054e-02 -2.97717545e-02\n",
            " -1.37868393e-02 -1.93663090e-02  2.84146070e-02  5.28641678e-02\n",
            " -3.49978171e-02  2.69893333e-02 -4.16262448e-02  5.94698265e-03\n",
            "  7.19807148e-02 -5.02187870e-02 -4.53740172e-03  1.83064491e-02\n",
            "  6.69207126e-02  3.03355306e-02  2.43252888e-02  3.41681466e-02\n",
            " -5.80411851e-02  1.65686086e-02 -6.35013357e-03  2.86970362e-02\n",
            " -2.87711360e-02  4.84555960e-04 -3.64969410e-02  2.90955491e-02\n",
            "  1.39887761e-02  7.81514421e-02  7.78379291e-03  4.14428562e-02\n",
            "  1.78857781e-02  3.35111842e-02  3.43819335e-02  6.37725741e-03\n",
            " -3.32499892e-02 -4.01718467e-02  1.72652192e-02 -1.34567115e-02\n",
            "  5.65021373e-02 -6.03745971e-03  4.03836183e-03 -7.09387381e-03\n",
            "  7.83107877e-02  9.26424563e-03  4.12370414e-02  5.61551675e-02\n",
            "  2.62945276e-02  4.64169867e-02 -1.16741220e-02  7.22844973e-02\n",
            "  3.21852863e-02 -3.72482128e-02 -2.29046121e-02 -1.81594901e-02\n",
            " -4.16987650e-02 -1.13531929e-02 -7.94504881e-02  9.32058319e-03\n",
            " -3.79760414e-02  5.07223718e-02 -3.75283100e-02  2.27734186e-02\n",
            " -1.18689463e-02 -6.02923073e-02  3.81510444e-02  2.70048343e-02\n",
            " -9.78136621e-03  5.11422753e-04 -3.02296132e-04  1.11916149e-02\n",
            "  1.22436341e-02 -5.94218150e-02 -8.13580304e-02 -7.17494562e-02\n",
            " -1.38714965e-02  2.23309956e-02  1.05421469e-02  3.93560380e-02\n",
            "  3.08106262e-02  4.84315790e-02  4.55128327e-02  4.14008163e-02\n",
            " -8.57080519e-02  1.54432505e-02  5.90965115e-02 -8.22660886e-03\n",
            " -3.92952487e-02  3.61345895e-03  8.22851881e-02 -1.82680711e-02\n",
            "  3.68821099e-02 -2.74527073e-03 -9.61237326e-02 -2.90055871e-02\n",
            " -6.52049929e-02  9.49145928e-02  1.37263425e-02 -1.89382508e-02\n",
            " -2.01456659e-02 -2.61279531e-02 -6.87475875e-03 -8.88958573e-04\n",
            "  7.72684440e-03  3.92589197e-02  4.31208611e-02  7.26675801e-03\n",
            " -1.18740797e-02  1.07450336e-02  2.42930837e-02 -5.83759323e-02\n",
            " -3.17303389e-02  3.89434434e-02 -2.25064568e-02 -1.48908570e-02\n",
            "  5.69715202e-02  2.36251578e-02  7.98758306e-03 -2.18979232e-02\n",
            "  3.99192050e-02  2.09209323e-03 -3.43616307e-03  7.88310543e-03\n",
            "  3.72551903e-02  4.57727313e-02 -1.35033727e-02  2.02423669e-02\n",
            "  3.96672413e-02  4.14480269e-02  4.47261482e-02  7.45143890e-02\n",
            "  7.15303794e-03 -1.79561786e-03  4.20099981e-02  8.99219438e-02\n",
            "  2.53121741e-02 -6.65632412e-02  1.03800967e-01  6.67081624e-02\n",
            "  3.16796303e-02  2.31144931e-02  4.06890251e-02 -7.88697600e-02\n",
            "  4.86360118e-02 -7.52510428e-02  1.22109856e-02  1.35928141e-02\n",
            "  8.63825344e-03 -3.75273526e-02 -6.06481582e-02  3.73095088e-02\n",
            " -5.93047142e-02  7.95400143e-02  4.55734059e-02  1.97233893e-02\n",
            " -3.49744894e-02 -3.52996439e-02  3.71975452e-03  1.08017605e-02\n",
            "  1.67709608e-02 -2.58406810e-02  1.01625174e-01  9.49562788e-02\n",
            "  3.71069089e-02 -4.40551117e-02 -3.59894633e-02  9.75552946e-03\n",
            " -4.25063372e-02  4.73074727e-02 -1.46056376e-02  3.25348713e-02\n",
            "  3.43113020e-03 -2.69246679e-02 -1.50085948e-02 -1.26295947e-02\n",
            " -1.33524425e-02  7.88139179e-03  1.33581720e-02 -5.77915385e-02\n",
            "  4.25438723e-03 -1.66563969e-02  4.78337109e-02 -9.39045846e-03\n",
            " -5.10251522e-02  2.68470123e-02  5.97400218e-03 -3.65606882e-03\n",
            "  9.63912159e-03 -5.37743568e-02  4.24188785e-02  4.78903763e-03\n",
            " -3.31187807e-02  3.40666510e-02 -2.01117229e-02 -4.54098508e-02\n",
            " -7.09850118e-02 -9.52303410e-04 -2.34565977e-02 -1.55305630e-02\n",
            "  1.01665892e-02  1.46941738e-02  5.93645610e-02 -1.53833758e-02\n",
            " -9.31352302e-02  1.20322667e-01 -2.30799243e-02 -5.74867725e-02\n",
            "  8.72961245e-05  2.33713053e-02 -1.67743918e-02  1.71000883e-03\n",
            "  1.03497729e-02 -2.93945596e-02  2.82955542e-02  1.52749065e-02\n",
            "  3.94202620e-02 -1.48854554e-02 -8.21793079e-03  5.96068576e-02\n",
            "  3.09557039e-02 -1.61244888e-02 -1.87514462e-02 -2.79975426e-03\n",
            " -3.41887623e-02  1.98955201e-02 -4.58954908e-02 -9.84932110e-02\n",
            "  2.94556525e-02  2.70123482e-02 -3.04265190e-02  1.85061246e-02\n",
            "  3.52927297e-02 -1.66029744e-02  3.84304412e-02 -5.48283085e-02\n",
            "  1.22363046e-02 -1.79236149e-03 -2.28773467e-02  4.67991605e-02\n",
            "  2.15711575e-02  4.91312779e-02  1.00772493e-02 -2.09001191e-02\n",
            "  1.34369135e-02  9.10896808e-04  4.93625365e-02  6.85106404e-03\n",
            "  3.84909660e-02  4.71308827e-02  1.46604367e-02 -3.51412445e-02\n",
            "  3.92441638e-02  7.97694549e-03 -7.49443397e-02  7.05835447e-02\n",
            "  2.61592492e-02  3.39831673e-02 -8.43209699e-02  4.08604480e-02\n",
            "  8.96111410e-03 -4.62821275e-02  1.23828650e-03  2.82005928e-02\n",
            "  3.37781906e-02 -1.80303529e-02  4.12942693e-02 -8.22533853e-03\n",
            "  3.91181521e-02 -5.39957806e-02  3.34856957e-02 -2.99630277e-02\n",
            "  1.83043610e-02  3.80874313e-02 -2.26299688e-02  6.27638623e-02\n",
            "  4.59758826e-02  6.26753084e-03  4.03507352e-02  5.64076193e-03]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JKbGHQGAM56",
        "colab_type": "text"
      },
      "source": [
        "## Finding closest word (cosine similarity)\n",
        "\n",
        "WEAT measures closeness of words by cosine similarity (vector in same direction, regardless of magnitude), and not by distance in multidimensional space. We're going to continue using that method to find the closest new word to our calculated vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F9IjMjJAhj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    len1 = np.linalg.norm(vec1)\n",
        "    len2 = np.linalg.norm(vec2)\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    return dot_product / (len1 * len2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6LR_SgBAndU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def closest_word(word, diff, printme=True):\n",
        "  # we have to take the original word out of contention; diff may be too small otherwise\n",
        "  original_id = tokenizer.encode(word)[1]\n",
        "\n",
        "  # make the diff adjustment\n",
        "  encoded = embedding_for_word(word)\n",
        "  new_word = encoded + diff\n",
        "\n",
        "  mostSim = 0\n",
        "  leastDistWord = -1\n",
        "  index = 0\n",
        "\n",
        "  for word in model.embeddings.word_embeddings.weight:\n",
        "    dist = cosine_similarity(new_word, word.detach().numpy())\n",
        "    if (dist > mostSim) and (index > 6) and (index != original_id):\n",
        "      mostSim = dist\n",
        "      leastDistWord = index\n",
        "    index += 1\n",
        "  if printme:\n",
        "    print(mostSim)\n",
        "\n",
        "  return tokenizer.decode([leastDistWord])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IurlOcxBA0ke",
        "colab_type": "text"
      },
      "source": [
        "Here we **take the article \"una\"** and reuse the ```article_diff``` vector to reach the male \"un\". Interestingly, the offset vector is very small and (in this uncased model) a diff of 0 would also give us \"un\" as the next available word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k414hTL9AwAg",
        "colab_type": "code",
        "outputId": "48d99ca0-7abd-4cd0-e569-896b64a2df8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "print(closest_word(\"una\", 1 * article_diff))\n",
        "print(closest_word(\"una\", 0 * article_diff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.74033797\n",
            "un\n",
            "0.7071701\n",
            "un\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXsjCf9tCWA7",
        "colab_type": "code",
        "outputId": "50ceb754-7d75-407a-e24e-34d0c10c060d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "print(closest_word(\"las\", 1 * noun_diff))\n",
        "print(closest_word(\"los\", -1 * noun_diff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5104903\n",
            "los\n",
            "0.506704\n",
            "las\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ8z0qVPBiP8",
        "colab_type": "text"
      },
      "source": [
        "**Trying some nouns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_V1XmqBBje9",
        "colab_type": "code",
        "outputId": "92290fde-2f2c-4e53-e693-87f73445f8d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(closest_word(\"compañera\", 1 * noun_diff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.65458435\n",
            "compañero\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zo0ZCkMBsGL",
        "colab_type": "code",
        "outputId": "d61abe10-0751-4cf0-cebc-f4ba54569ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "print(closest_word(\"doctor\", -1 * noun_diff))\n",
        "print(closest_word(\"doctora\", 1 * noun_diff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.53279793\n",
            "doctora\n",
            "0.51159954\n",
            "doctor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks5LkDeqB2D3",
        "colab_type": "text"
      },
      "source": [
        "**On a word (library) where there is not a flipped gender word**, we get a capitalized word. I tried this same experiment with an all-lowercase model and biblioteca's next neighbor was the plural, bibliotecas. Unfortunately the all-lowercase model caused other problems with my tests, so I returned to the cased model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBds4xJjB8uf",
        "colab_type": "code",
        "outputId": "a8a0d216-a627-43b7-cd46-8ededd9e32bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(closest_word(\"biblioteca\", 1 * noun_diff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.45826063\n",
            "Biblioteca\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSaMHGKRpuj3",
        "colab_type": "text"
      },
      "source": [
        "**On plurals** - the answers are there, but the diff vector is hard to come by"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6q9Pcq_qOvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plural_diff = embedding_for_word(\"maestros\") - embedding_for_word(\"maestras\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKG4USETpwoK",
        "colab_type": "code",
        "outputId": "6889b004-0693-4761-db55-5f6739fcb765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "print(closest_word(\"chicos\", -0.5 * plural_diff))\n",
        "print(closest_word(\"hombres\", -0.9 * plural_diff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5572855\n",
            "chicas\n",
            "0.4703354\n",
            "hombre\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8dhpr2zqJCX",
        "colab_type": "code",
        "outputId": "1a2528c4-f2b8-4bd0-f62a-603bf970cded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(closest_word(\"madres\", 0.6 * plural_diff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47073933\n",
            "padres\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdh9BAXgCBh8",
        "colab_type": "text"
      },
      "source": [
        "Trying it on names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Grgg6cCCO3",
        "colab_type": "code",
        "outputId": "e5a0b380-4e1f-45f6-c41d-6b5a6ec04978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "print(closest_word(\"Paula\", 1 * noun_diff))\n",
        "print(closest_word(\"Cecilia\", 1 * noun_diff))\n",
        "\n",
        "# Male names flip to Maestra??\n",
        "print(closest_word(\"Pablo\", -1 * noun_diff))\n",
        "print(closest_word(\"Nicolás\", -1 * noun_diff))\n",
        "# same for random text\n",
        "print(closest_word(\"d22n9@j\", -1 * noun_diff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3014029\n",
            "Bruno\n",
            "0.39872557\n",
            "Claudio\n",
            "0.48295468\n",
            "maestra\n",
            "0.46040207\n",
            "maestra\n",
            "0.4352325\n",
            "maestra\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8spi_3_eCOcQ",
        "colab_type": "code",
        "outputId": "803565ef-8c67-4d25-a132-de8b39c6baa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# making a male name more male?\n",
        "print(closest_word(\"Eduardo\", 1 * noun_diff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.37805256\n",
            "Ricardo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9ybo8m0CeXP",
        "colab_type": "text"
      },
      "source": [
        "## Flipping names with gender-guesser package\n",
        "Flipping male -> female names is proving unreliable, so I will use this module to detect names, estimate most likely gender for the name, and then flip to a name from the opposite gender.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2ujJ3U8CgpM",
        "colab_type": "code",
        "outputId": "c64f8a2a-9b6b-44d2-dc36-a35eeb4ad8b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "! pip install gender-guesser\n",
        "! wget https://github.com/lead-ratings/gender-guesser/blob/master/gender_guesser/data/nam_dict.txt?raw=true"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gender-guesser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/fb/3f2aac40cd2421e164cab1668e0ca10685fcf896bd6b3671088f8aab356e/gender_guesser-0.4.0-py2.py3-none-any.whl (379kB)\n",
            "\r\u001b[K     |▉                               | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 1.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 92kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 102kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 143kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 184kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 194kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 204kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 235kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 245kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 256kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 276kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 286kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 296kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 327kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 337kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 348kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 358kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 368kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 389kB 2.6MB/s \n",
            "\u001b[?25hInstalling collected packages: gender-guesser\n",
            "Successfully installed gender-guesser-0.4.0\n",
            "--2020-05-14 01:27:07--  https://github.com/lead-ratings/gender-guesser/blob/master/gender_guesser/data/nam_dict.txt?raw=true\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/lead-ratings/gender-guesser/raw/master/gender_guesser/data/nam_dict.txt [following]\n",
            "--2020-05-14 01:27:08--  https://github.com/lead-ratings/gender-guesser/raw/master/gender_guesser/data/nam_dict.txt\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/lead-ratings/gender-guesser/master/gender_guesser/data/nam_dict.txt [following]\n",
            "--2020-05-14 01:27:09--  https://raw.githubusercontent.com/lead-ratings/gender-guesser/master/gender_guesser/data/nam_dict.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4351271 (4.1M) [text/plain]\n",
            "Saving to: ‘nam_dict.txt?raw=true’\n",
            "\n",
            "nam_dict.txt?raw=tr 100%[===================>]   4.15M  18.0MB/s    in 0.2s    \n",
            "\n",
            "2020-05-14 01:27:10 (18.0 MB/s) - ‘nam_dict.txt?raw=true’ saved [4351271/4351271]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgBzKbCOCoqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gender_guesser.detector as gender\n",
        "d = gender.Detector()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKDQlY7ICq5R",
        "colab_type": "code",
        "outputId": "ff4b0a1c-5d0c-4037-db5b-07ff996d167a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(d.get_gender(\"Pablo\"))\n",
        "print(d.get_gender(\"Paula\"))\n",
        "print(d.get_gender(\"Juan\"))\n",
        "print(d.get_gender(\"José\"))\n",
        "print(d.get_gender(\"Ashley\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "male\n",
            "female\n",
            "male\n",
            "male\n",
            "mostly_female\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2egdUJTCwMx",
        "colab_type": "text"
      },
      "source": [
        "### Using the underlying data to generate names\n",
        "\n",
        "Luckily gender-guesser includes information about popularity of names in Spain and a few other countries. We can then generate a male or female name from a list, and likely find it in the BETO model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvWX2el0CvuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recc_names = {'M': [], 'F': []}\n",
        "\n",
        "with open(\"nam_dict.txt?raw=true\", \"r\") as names:\n",
        "  found_names = 0\n",
        "  for name in names:\n",
        "    if name[0] == \"#\":\n",
        "      # readme\n",
        "      continue\n",
        "    spanish_pop = name[36]\n",
        "    if spanish_pop != \" \" and spanish_pop > \"3\":\n",
        "      #print(spanish_pop)\n",
        "      conventional_binary_gender = name.split(' ')[0]\n",
        "      if conventional_binary_gender in ['M', 'F']:\n",
        "        name = name.split(' ')[2]\n",
        "        recc_names[conventional_binary_gender].append(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxVp9TYxjwNo",
        "colab_type": "code",
        "outputId": "f2b68bba-bbce-4edc-a6f9-4f53428c950c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(recc_names['M'][0:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Adolfo', 'Adrián', 'Agustín', 'Alberto', 'Alejandro', 'Alfonso', 'Alfredo', 'Álvaro', 'Amador', 'Anastasio']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWMVWlJRC5qS",
        "colab_type": "text"
      },
      "source": [
        "# Multilingual BERT\n",
        "I've wondered if it would make sense to use mBERT instead of BETO for this task. Here I'm using the all-lowercase model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uWNU-6IC-bF",
        "colab_type": "code",
        "outputId": "cc3c84b6-576b-4eab-ef6f-087306e29db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9e25cd042b0849ee8599a99994c7dd05",
            "b0511af18b1f4ced891e971502764f16",
            "7f33cdffd5294b14b0d3cc4a0e85726e",
            "a886f23e617145819374c9fae072ec36",
            "07415d2170694d7fb31818be2cdacd0c",
            "3844fce147ee4fe0a9660bdd6ef969fd",
            "9c599524f3854d9ba2b6739b5fe15d6a",
            "415ab20baef74fdabfd516bd71045560",
            "d23ac3865b38424cbc8ad5a7fc62f4da",
            "a5858dfbb9b342ce8338a424e25348c8",
            "e4d3a33d9a1645729e3c2d2e4da1df1c",
            "04f1a2458f1b490a949123e8df6abd27",
            "0b9e8bc4d50d48f2a5efed9e3e7e2b53",
            "8619a39f517c460ebf3c8db3978e86e6",
            "0b3108c12bf84a9cb5800e4a1063f2e6",
            "1defe64ca93a461e89dd246af7700b4b",
            "dbddac59a106422b9bdba4b26d3e7a01",
            "6733513710034fcda907736a4f770441",
            "fa4d344f7c404370865dbac6d0a2da81",
            "7feec931ab12437394bd5f73de31b8ae",
            "674794f3bccd426b9338412c71f3b932",
            "e68fe8e9d0d845739f6cdadcdfbd72ec",
            "0bdf2ff93b254b829dfd6dd62402513b",
            "6dd163f312d04be69aca332960b506c6"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer2 = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
        "\n",
        "model2 = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
        "model2.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e25cd042b0849ee8599a99994c7dd05",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d23ac3865b38424cbc8ad5a7fc62f4da",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbddac59a106422b9bdba4b26d3e7a01",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=672271273.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IZS-m0eDCDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_for_bert(word):\n",
        "  id = tokenizer2.encode(word)[1]\n",
        "  return model2.embeddings.word_embeddings.weight[id].detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JwN7NHDDEBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def closest_in_bert(word, diff):\n",
        "  original_id = tokenizer2.encode(word)[1]\n",
        "  encoded = embedding_for_bert(word)\n",
        "  new_word = encoded + diff\n",
        "\n",
        "  leastDist = 100\n",
        "  leastDistWord = -1\n",
        "  mostSim = 0\n",
        "\n",
        "  index = 0\n",
        "  for word in model2.embeddings.word_embeddings.weight:\n",
        "    dist = cosine_similarity(new_word, word.detach().numpy())\n",
        "    if (dist > mostSim) and (index > 6) and (index != original_id):\n",
        "      mostSim = dist\n",
        "      leastDistWord = index\n",
        "    index += 1\n",
        "  print(mostSim)\n",
        "\n",
        "  return tokenizer2.decode([leastDistWord])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n_4x7JBFqcX",
        "colab_type": "text"
      },
      "source": [
        "### English language analogies / gender flip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_L93WcIDM9G",
        "colab_type": "text"
      },
      "source": [
        "English analogies don't work so well in mBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPYAlmgBFpwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_n_diff = embedding_for_bert(\"man\") - embedding_for_bert(\"woman\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDUmGLlEDQVf",
        "colab_type": "code",
        "outputId": "c8347553-8f18-4244-a09a-85b3a10a551d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "closest_in_bert(\"king\", -1 * bert_n_diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5023459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'woman'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtrLyWlgDSFl",
        "colab_type": "code",
        "outputId": "cfccddca-2452-459e-f795-da0647e73e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "closest_in_bert(\"queen\", 1 * bert_n_diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.60389835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'man'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjYwJ6HUDVJk",
        "colab_type": "text"
      },
      "source": [
        "### Spanish language analogies / gender flip\n",
        "\n",
        "Same problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBMazEl-DYAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_es_n_diff = embedding_for_bert(\"hombre\") - embedding_for_bert(\"mujer\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qr-owZJDaL3",
        "colab_type": "code",
        "outputId": "426cd90e-1323-42dd-fe29-16318592f7a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "closest_in_bert(\"rey\", -1 * bert_es_n_diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.44842014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mujer'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWmsNHVpF1tT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gender_es = embedding_for_bert(\"hombre\") - embedding_for_bert(\"mujer\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYTjpBcjGgZ5",
        "colab_type": "text"
      },
      "source": [
        "This one appears to work; unfortunately ñ is being dropped"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byisZxR5F3Dm",
        "colab_type": "code",
        "outputId": "fc4d38bd-20a4-4617-89c7-afbc56419b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "closest_in_bert(\"compañera\", 1 * gender_es)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'companero'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpFnPsP-Dahw",
        "colab_type": "text"
      },
      "source": [
        "### Translation\n",
        "\n",
        "Translation works, though!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56vfmmUEDe54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate_en_es = embedding_for_bert(\"biblioteca\") - embedding_for_bert(\"library\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSODltF0DgZz",
        "colab_type": "code",
        "outputId": "136ea64a-35dc-47e9-9341-72bb7e479f20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "closest_in_bert(\"escuela\", -1 * translate_en_es)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.529695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'school'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F-Rgw9fG2m4",
        "colab_type": "text"
      },
      "source": [
        "### Reflections\n",
        "\n",
        "It's unfair to expect perfect single-word changes with sentence-level transformers.\n",
        "\n",
        "I would like to try this as seq2seq, but it's beyond my level in the here and now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vcXGMVEHcar",
        "colab_type": "text"
      },
      "source": [
        "# Parse and flip sentences with spaCy\n",
        "\n",
        "Here's the strategy: parse Spanish sentences, determine which words need to be flipped, and use BETO to flip their corresponding articles and adjectives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zSGBl17Hrif",
        "colab_type": "code",
        "outputId": "68adfdc8-fadd-4208-de41-c5579c3bc472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "! pip install --upgrade spacy\n",
        "! python -m spacy download es_core_news_md"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.1.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Collecting es_core_news_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.2.5/es_core_news_md-2.2.5.tar.gz (78.4MB)\n",
            "\u001b[K     |████████████████████████████████| 78.4MB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from es_core_news_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_md==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_md==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: es-core-news-md\n",
            "  Building wheel for es-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-md: filename=es_core_news_md-2.2.5-cp36-none-any.whl size=79649483 sha256=970a3d7deec88b36e340626ef6af2b594426ad19005d0327f8ca4c43b1acadef\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r9p8u3bg/wheels/b7/bb/a3/29ab5cf80c2c0a8fa0f2af8402fdace3f159e8265f0fdcbcdb\n",
            "Successfully built es-core-news-md\n",
            "Installing collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omG1NvjAH_Gm",
        "colab_type": "text"
      },
      "source": [
        "**Restart runtime after you install that stuff ^^**\n",
        "\n",
        "## Explore dependency parsing\n",
        "\n",
        "The key to spaCy here is their dependency parsing on https://spacy.io/usage/linguistic-features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji2Tacl-Ht0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"es_core_news_md\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipZprAHGHwWe",
        "colab_type": "code",
        "outputId": "dced2d76-ee1f-4110-b83c-2a06fb6d026b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "doc1 = nlp(\"Estamos en nuestra casa.\")\n",
        "for chunk in doc1.noun_chunks:\n",
        "    print(chunk.text + \"\\n\" + chunk.root.text + \"\\n\" + chunk.root.dep_ + \"\\n\" + chunk.root.head.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nuestra casa\n",
            "casa\n",
            "ROOT\n",
            "casa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQDEisHvHyb1",
        "colab_type": "code",
        "outputId": "ace1a90d-1edd-4e18-cde4-bfe531642bcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "doc = nlp(\"La mujer y doctor van a la biblioteca para leer un libro viejo.\")\n",
        "for token in doc:\n",
        "     print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "            [child for child in token.children])\n",
        "\n",
        "def get_gender_deps(sentence):\n",
        "    pairings = []\n",
        "    for token in sentence:\n",
        "        if type(token) != type(''):\n",
        "            if token.dep_ == \"det\" or token.dep_ == \"amod\":\n",
        "                pairings.append([token.text, token.head.text])\n",
        "    return pairings\n",
        "\n",
        "print('---')\n",
        "print('\\n'.join(list(map((lambda line: line[0] + \" depends on gender of \" + line[1]), get_gender_deps(doc)))))\n",
        "print('---')\n",
        "print('\\n'.join(list(map((lambda line: line[0] + \" depends on gender of \" + line[1]), get_gender_deps(doc1)))))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La det mujer NOUN []\n",
            "mujer nsubj van VERB [La, doctor]\n",
            "y cc doctor NOUN []\n",
            "doctor conj mujer NOUN [y]\n",
            "van ROOT van VERB [mujer, biblioteca, leer, .]\n",
            "a case biblioteca NOUN []\n",
            "la det biblioteca NOUN []\n",
            "biblioteca obj van VERB [a, la]\n",
            "para mark leer VERB []\n",
            "leer advcl van VERB [para, libro]\n",
            "un det libro NOUN []\n",
            "libro obj leer VERB [un, viejo]\n",
            "viejo amod libro NOUN []\n",
            ". punct van VERB []\n",
            "---\n",
            "La depends on gender of mujer\n",
            "la depends on gender of biblioteca\n",
            "un depends on gender of libro\n",
            "viejo depends on gender of libro\n",
            "---\n",
            "nuestra depends on gender of casa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpe7HLi_Hdp3",
        "colab_type": "text"
      },
      "source": [
        "Here's a sentence directly from Spanish Wikipedia. It doesn't have any words which we'd change, but we can verify that it is parsed correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S79Ph5tLEXXF",
        "colab_type": "code",
        "outputId": "97057dbc-14cf-446e-9660-ee328a8e9c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "wikiSentence = \"Las personas interesadas consideran que la tendencia de las lenguas a cambiar en su desarrollo natural a través de la historia, permite potencialmente lograr una mayor inclusión social, cuando cierta conciencia social influye sobre los cambios de las lenguas.\"\n",
        "get_gender_deps(nlp(wikiSentence))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Las', 'personas'],\n",
              " ['interesadas', 'personas'],\n",
              " ['la', 'tendencia'],\n",
              " ['las', 'lenguas'],\n",
              " ['su', 'desarrollo'],\n",
              " ['natural', 'desarrollo'],\n",
              " ['la', 'historia'],\n",
              " ['una', 'inclusión'],\n",
              " ['mayor', 'inclusión'],\n",
              " ['social', 'inclusión'],\n",
              " ['cierta', 'conciencia'],\n",
              " ['social', 'conciencia'],\n",
              " ['los', 'cambios'],\n",
              " ['las', 'lenguas']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lcqSPAmIISq",
        "colab_type": "text"
      },
      "source": [
        "These sentences are interesting... they identify \"el\" is connected to lingüista, but this is more about the name/subject; the article would be \"la\" for a female linguist.\n",
        "\n",
        "In the next sentence, \"holandés\" would change to \"holandesa\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0spWUA7HvRS",
        "colab_type": "code",
        "outputId": "bb69777b-225c-42f4-8c8b-f5e49e2ea56b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "wikiSentence2 = \"En el siglo XX el lingüista estadounidense Noam Chomsky creó la corriente conocida como generativismo.\"\n",
        "get_gender_deps(nlp(wikiSentence2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['el', 'siglo'],\n",
              " ['el', 'lingüista'],\n",
              " ['estadounidense', 'lingüista'],\n",
              " ['la', 'corriente'],\n",
              " ['conocida', 'corriente']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxfTeVJVIbP9",
        "colab_type": "code",
        "outputId": "ed9a9f7f-732f-4f23-a29d-7c161b0316f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "wikiSentence3 = \"La figura más relevante dentro de esta corriente tal vez sea el lingüista holandés Simon C. Dik, autor del libro Functional Grammar.\"\n",
        "get_gender_deps(nlp(wikiSentence3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['La', 'figura'],\n",
              " ['relevante', 'figura'],\n",
              " ['esta', 'corriente'],\n",
              " ['el', 'lingüista'],\n",
              " ['holandés', 'lingüista']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FgiblpCKJ1u",
        "colab_type": "text"
      },
      "source": [
        "The noun \"miembro\" here does not need to change for a female subject.  This is common for a lot of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpk1ti4yJKkv",
        "colab_type": "code",
        "outputId": "5a390517-e775-4caa-903b-c31e853e9d83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "wikiSentence4 = \"Tuvo una influencia decisiva en la creación de la Cruz Roja Británica en 1870, y fue miembro de su comité de damas interesándose por las actividades del movimiento hasta su fallecimiento.\"\n",
        "get_gender_deps(nlp(wikiSentence4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['una', 'influencia'],\n",
              " ['decisiva', 'influencia'],\n",
              " ['la', 'creación'],\n",
              " ['la', 'Cruz'],\n",
              " ['su', 'comité'],\n",
              " ['las', 'actividades'],\n",
              " ['su', 'fallecimiento']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJLcYXcIKmA8",
        "colab_type": "code",
        "outputId": "f2ba46d6-4a52-4e2a-e7e8-8e79179e7c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "wikiSentence5 = \"El padre de Fanny (abuelo materno de Florence) fue el abolicionista y unitarista William Smith.\"\n",
        "get_gender_deps(nlp(wikiSentence5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['El', 'padre'], ['materno', 'abuelo'], ['el', 'abolicionista']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AITUtdakM1dt",
        "colab_type": "text"
      },
      "source": [
        "## Come up with a strategy for flipping words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P24aS55XKz3z",
        "colab_type": "code",
        "outputId": "c87041f4-12ec-4681-9295-67d405508bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(closest_word(\"El\", 0 * article_diff))\n",
        "print(closest_word(\"padre\", 0 * noun_diff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6407496\n",
            "La\n",
            "0.6534688\n",
            "madre\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWTY5LhmLBIq",
        "colab_type": "code",
        "outputId": "24142c41-f04c-4598-8b04-0b53d9769e1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(closest_word(\"materno\", 0 * noun_diff))\n",
        "print(closest_word(\"abuelo\", 0 * noun_diff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.60393536\n",
            "materna\n",
            "0.6682723\n",
            "abuela\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yw2Wi3bLNX2",
        "colab_type": "code",
        "outputId": "b2b3d38c-6420-4945-b528-4fb0e5a6c75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(closest_word(\"el\", 0 * article_diff))\n",
        "print(closest_word(\"abolicionista\", 0 * noun_diff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.63179874\n",
            "la\n",
            "0.5781752\n",
            "abolición\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R159wlT4Msrj",
        "colab_type": "text"
      },
      "source": [
        "### Flip everything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTcYVmd1Mrzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flip_sentence(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    pairings = get_gender_deps(doc)\n",
        "    words = []\n",
        "    for token in doc:\n",
        "        alt_word = None\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            alt_word = closest_word(token.text, 0.5 * noun_diff, printme=False)\n",
        "        elif len(pairings) > 0 and token.text == pairings[0][0]:\n",
        "            diff = noun_diff\n",
        "            if token.text.lower() in ['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'estas', 'estes', 'estos', 'aquello', 'aquella', 'aquellos', 'aquellas']:\n",
        "              diff = article_diff\n",
        "            alt_word = closest_word(token.text, 1 * diff, printme=False)\n",
        "            pairings = pairings[1:]\n",
        "        if alt_word is None or alt_word.lower() == token.text.lower():\n",
        "            words.append(token.text)\n",
        "        else:\n",
        "            words.append(alt_word)\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obJX7uerNLz-",
        "colab_type": "code",
        "outputId": "86ff5322-12b0-4095-d8ac-e8a547069f96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "flip_sentence(\"Montevideo es la ciudad más importante en lo que a deportes se refiere de todo el Uruguay\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Montevideo es el ciudades más importantes en lo que a deporte se refiere de todo el Uruguay'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-cVQkFwRa52",
        "colab_type": "text"
      },
      "source": [
        "### Don't flip if closest neighbor to noun is a plural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHp5L2OiRdn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flip_2(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    pairings = get_gender_deps(doc)\n",
        "    words = []\n",
        "    for token in doc:\n",
        "        alt_word = None\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            if 'Gender=Masc' in token.tag_:\n",
        "              alt_word = closest_word(token.text, -0.5 * noun_diff, printme=False)\n",
        "            else:\n",
        "              alt_word = closest_word(token.text, 0.5 * noun_diff, printme=False)\n",
        "            alt_nlp = nlp(alt_word)[0]\n",
        "\n",
        "            if ('NOUN__Gender=Fem' in token.tag_ and 'NOUN__Gender=Fem' in alt_nlp.tag_):\n",
        "              alt_word = None\n",
        "            elif ('NOUN__Gender=Masc' in token.tag_ and 'NOUN__Gender=Masc' in alt_nlp.tag_):\n",
        "              alt_word = None\n",
        "            \n",
        "        elif len(pairings) > 0 and token.text == pairings[0][0]:\n",
        "            diff = noun_diff\n",
        "            if token.text.lower() in ['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'estas', 'estes', 'estos', 'aquello', 'aquella', 'aquellos', 'aquellas']:\n",
        "              diff = article_diff\n",
        "            \n",
        "\n",
        "            dep_noun_nlp = nlp(pairings[0][1])\n",
        "            if 'Gender=Masc' in dep_noun_nlp[0].tag_:\n",
        "              alt_noun = closest_word(pairings[0][1], -0.5 * noun_diff, printme=False)\n",
        "            else:\n",
        "              alt_noun = closest_word(pairings[0][1], 0.5 * noun_diff, printme=False)\n",
        "            alt_noun_nlp = nlp(alt_noun)\n",
        "\n",
        "            if ('NOUN__Gender=Fem' in dep_noun_nlp[0].tag_ and 'NOUN__Gender=Fem' in alt_noun_nlp[0].tag_):\n",
        "              alt_word = None\n",
        "            elif ('NOUN__Gender=Masc' in dep_noun_nlp[0].tag_ and 'NOUN__Gender=Masc' in alt_noun_nlp[0].tag_):\n",
        "              alt_word = None\n",
        "            elif ('PROPN_' in dep_noun_nlp[0].tag_):\n",
        "              alt_word = None\n",
        "            else:\n",
        "              if 'Gender=Masc' in token.tag_:\n",
        "                alt_word = closest_word(token.text, -1 * diff, printme=False)\n",
        "              else:\n",
        "                alt_word = closest_word(token.text, 1 * diff, printme=False)\n",
        "            pairings = pairings[1:]\n",
        "        if alt_word is None or alt_word.lower() == token.text.lower():\n",
        "            words.append(token.text)\n",
        "        else:\n",
        "            words.append(alt_word)\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_YUznTERzzn",
        "colab_type": "code",
        "outputId": "c95ab711-c278-46b3-98eb-82b906c1079b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "flip_2(\"Montevideo es la ciudad más importante en lo que a deportes se refiere de todo el Uruguay\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Montevideo es la ciudad más importante en lo que a deportes se refiere de todo el Uruguay'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbBBkOmGT304",
        "colab_type": "code",
        "outputId": "dc0b402e-3f6a-4da9-9e61-9a8676a8460f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "flip_2(\"El padre de Fanny (abuelo materno de Florence) fue el abolicionista y unitarista William Smith.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'La madre de Fanny ( abuela materna de Florence ) fue la abolición y unitarista William Smith .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMBhqc27XQsY",
        "colab_type": "text"
      },
      "source": [
        "Some unusual lateral jumps here: different years, \"House\" and \"Chase\", a synonym for rivalidad (because it did have opposing gender)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_rjJXFeWML5",
        "colab_type": "code",
        "outputId": "1b7cb53a-24e9-40dc-e430-4775581d5c2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "flip_2(\"Uno de los aspectos más destacados de la rivalidad fue la llamada Maldición del Bambino, un periodo de 86 años (1918-2004) en el que los Red Sox no ganaron ni una vez la Serie Mundial.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Uno de los aspectos más destacados de el enfrentamientos fue el llamado Maldición del Bambino , un periodo de 86 años ( 1916 - 2005 ) en la que los Red Sox no ganaron ni una vez la Serie Mundial .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMX6SQwpW2Gg",
        "colab_type": "code",
        "outputId": "81247f1e-498e-4d59-d91f-e8dbe9a3bc05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "flip_2(\"El Dr. House a menudo confronta con su jefa, la administradora del hospital Dra. Lisa Cuddy (Lisa Edelstein), y a su equipo de diagnóstico, debido a la gran cantidad de hipótesis que surgen con respecto a la enfermedad del paciente basadas en finas o controvertidas perspicacias.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'La Dr . Chase a veces confronta con sus jefe , el administradores del hospital Dra . Lisa Cuddy ( Lisa Edelstein ) , y a su equipo de diagnóstico , debido a la gran cantidad de hipótesis que surgen con respecto a la enfermedad del pacientes basados en fino o controvertidas perspicacias .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF0yqH7LhJr4",
        "colab_type": "text"
      },
      "source": [
        "## Correct errors with numbers, verbs, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mOPSiZGXofy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def better_gender_deps(sentence):\n",
        "    pairings = []\n",
        "    for token in sentence:\n",
        "        if type(token) != type(''):\n",
        "            if token.dep_ == \"det\" or token.dep_ == \"amod\":\n",
        "                pairings.append([token, token.head])\n",
        "    return pairings\n",
        "\n",
        "def flip_noun(og_token):\n",
        "    if 'Gender=Masc' in og_token.tag_:\n",
        "      alt_word = closest_word(og_token.text, -0.5 * noun_diff, printme=False)\n",
        "    else:\n",
        "      alt_word = closest_word(og_token.text, 0.5 * noun_diff, printme=False)\n",
        "\n",
        "    print(og_token.text + \" -> \" + alt_word)\n",
        "\n",
        "    alt_nlp = nlp(alt_word)[0]\n",
        "    if ((alt_nlp.pos_ != 'NOUN') or # don't allow change to a verb\n",
        "        ('NOUN__Gender=Fem' in og_token.tag_ and 'NOUN__Gender=Fem' in alt_nlp.tag_) or\n",
        "        ('NOUN__Gender=Masc' in og_token.tag_ and 'NOUN__Gender=Masc' in alt_nlp.tag_)): # or\n",
        "        #(og_token.lemma_ not in alt_nlp.lemma_)):\n",
        "        alt_word = None\n",
        "    return alt_word\n",
        "\n",
        "def flip_3(sentence):\n",
        "    sentence = sentence.replace('Dr.', 'Doctor').replace('Dra.', 'Doctora').replace('Sr.', 'Señor').replace('Sra.', 'Señora').replace('Srta.', 'Señorita')\n",
        "    doc = nlp(sentence)\n",
        "    pairings = better_gender_deps(doc)\n",
        "    words = []\n",
        "    for token in doc:\n",
        "        alt_word = None\n",
        "\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            if 'AdvType=Tim' not in token.tag_: # don't change years\n",
        "              alt_word = flip_noun(token)\n",
        "              if alt_word is not None:\n",
        "                print(token.text + \" ? \" + str(alt_word))\n",
        "            \n",
        "        elif len(pairings) > 0 and token.text == pairings[0][0].text:\n",
        "            diff = noun_diff\n",
        "            if token.text.lower() in ['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'estas', 'estes', 'estos', 'aquello', 'aquella', 'aquellos', 'aquellas']:\n",
        "              diff = article_diff\n",
        "            \n",
        "            dep_noun_token = pairings[0][1]\n",
        "            if (('PROPN_' not in dep_noun_token.tag_) and ('Gender' in token.tag_)):\n",
        "              alt_noun = flip_noun(dep_noun_token)\n",
        "              if alt_noun is not None: # don't change ADJ if the noun would not change\n",
        "                print(token.text + \" ? \" + str(alt_noun))\n",
        "                alt_noun_nlp = nlp(alt_noun)\n",
        "                if 'Gender=Masc' in token.tag_:\n",
        "                  alt_word = closest_word(token.text, -0.6 * diff, printme=False)\n",
        "                else:\n",
        "                  alt_word = closest_word(token.text, 0.6 * diff, printme=False)\n",
        "            pairings = pairings[1:]\n",
        "        if alt_word is None or alt_word.lower() == token.text.lower():\n",
        "            words.append(token.text)\n",
        "        else:\n",
        "            words.append(alt_word)\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khj3BHKoY-hF",
        "colab_type": "code",
        "outputId": "384d83c0-5ec8-46f6-e9d1-cfe20a857b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "flip_3(\"El Dr. House a menudo confronta con su jefa, la administradora del hospital Dra. Lisa Cuddy (Lisa Edelstein), y a su equipo de diagnóstico, debido a la gran cantidad de hipótesis que surgen con respecto a la enfermedad del paciente basadas en finas o controvertidas perspicacias.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "menudo -> veces\n",
            "menudo ? veces\n",
            "jefa -> jefe\n",
            "jefa ? jefe\n",
            "administradora -> administradores\n",
            "la ? administradores\n",
            "administradora -> administradores\n",
            "administradora ? administradores\n",
            "hospital -> Hospital\n",
            "equipo -> equipos\n",
            "diagnóstico -> diagnos\n",
            "cantidad -> cantidades\n",
            "cantidad -> cantidades\n",
            "hipótesis -> teorías\n",
            "respecto -> Respecto\n",
            "respecto ? Respecto\n",
            "enfermedad -> enfermedades\n",
            "enfermedad -> enfermedades\n",
            "paciente -> pacientes\n",
            "paciente ? pacientes\n",
            "paciente -> pacientes\n",
            "basadas ? pacientes\n",
            "finas -> fino\n",
            "perspicacias -> persecución\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'El Doctor House a veces confronta con su jefe , el administradores del hospital Doctora Lisa Cuddy ( Lisa Edelstein ) , y a su equipo de diagnóstico , debido a la gran cantidad de hipótesis que surgen con respecto a la enfermedad del pacientes basados en finas o controvertidas perspicacias .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3hVDqJWdOFg",
        "colab_type": "code",
        "outputId": "276c68fb-1e4f-4288-cb2f-be521c50d5a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "flip_3(\"En 2004 su creador David Shore y los productores ejecutivos Katie Jacobs y Paul Attanasio, le presentaron a la cadena televisiva Fox Broadcasting Company\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'En 2004 su creador David Shore y los productores ejecutivos Katie Jacobs y Paul Attanasio , le presentaron a la cadena televisiva Fox Broadcasting Company'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAewT9lHePtf",
        "colab_type": "code",
        "outputId": "1ebfff28-b71b-4a9c-f630-89dd74815428",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "flip_3(\"Durante las audiciones, el actor británico Hugh Laurie se encontraba en las filmaciones de la película El vuelo del Fénix en Namibia.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "el ? actriz\n",
            "actor ? actriz\n",
            "británico ? actriz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Durante las audiciones , la actriz británica Hugh Laurie se encontraba en las filmaciones de la película El vuelo del Fénix en Namibia .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w79RRou9hQBR",
        "colab_type": "text"
      },
      "source": [
        "## Final proper noun / name flip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FOpv9AH1LEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pronoun_diff = embedding_for_word(\"nosotros\") - embedding_for_word(\"nosotras\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL5fakPpkrvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random \n",
        "\n",
        "import gender_guesser.detector as gender\n",
        "d = gender.Detector()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5F_zhx3hdXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pub_gender_deps(sentence):\n",
        "    pairings = []\n",
        "    for token in sentence:\n",
        "        if type(token) != type(''):\n",
        "            if token.dep_ == \"det\" or token.dep_ == \"amod\":\n",
        "                pairings.append([token, token.head])\n",
        "    return pairings\n",
        "\n",
        "def pub_flip_noun(og_token):\n",
        "    if og_token.pos_ == \"PRON\": # pronouns hit different (nosotros/nosotras)\n",
        "      diff = 1 * noun_diff #pronoun_diff\n",
        "    else:\n",
        "      diff = 0.5 * noun_diff\n",
        "\n",
        "    if (og_token.text.lower() == \"nosotros\"): # hardcoded b/c of weird spaCy parsing on upper/lower case\n",
        "      return 'nosotras'\n",
        "    elif (og_token.text.lower() == \"nosotras\"):\n",
        "      return 'nosotros'\n",
        "    elif 'Gender=Masc' in og_token.tag_:\n",
        "      # flip to feminine\n",
        "      alt_word = closest_word(og_token.text, -1 * diff, printme=False)\n",
        "    elif (og_token.pos_ == \"NOUN\") and (\"Gender\" not in og_token.tag_):\n",
        "      # el/la lingüista - word stays the same but adj. should change\n",
        "      # don't try this with pronouns; I dunno how they are tagged\n",
        "      alt_word = og_token.text\n",
        "    else:\n",
        "      # flip to masculine\n",
        "      alt_word = closest_word(og_token.text, 1 * diff, printme=False)\n",
        "\n",
        "    alt_nlp = nlp(alt_word)[0]\n",
        "    if ((alt_nlp.pos_ not in ['NOUN', 'PRON']) or # don't allow change to a verb\n",
        "        ('NOUN__Gender=Fem' in og_token.tag_ and 'NOUN__Gender=Fem' in alt_nlp.tag_) or\n",
        "        ('NOUN__Gender=Masc' in og_token.tag_ and 'NOUN__Gender=Masc' in alt_nlp.tag_)): # or\n",
        "        #(og_token.lemma_ not in alt_nlp.lemma_)):\n",
        "        alt_word = None\n",
        "    return alt_word\n",
        "\n",
        "def pub_flip(sentence):\n",
        "    sentence = sentence.replace('Dr.', 'Doctor').replace('Dra.', 'Doctora').replace('Sr.', 'Señor').replace('Sra.', 'Señora').replace('Srta.', 'Señorita')\n",
        "    doc = nlp(sentence)\n",
        "    pairings = pub_gender_deps(doc)\n",
        "    words = []\n",
        "    just_saw_proper_noun = False\n",
        "\n",
        "    for token in doc:\n",
        "        alt_word = None\n",
        "\n",
        "        if token.pos_ == \"PROPN\" and not just_saw_proper_noun: # swap first names\n",
        "            conventional_binary_gen = d.get_gender(token.text)\n",
        "            just_saw_proper_noun = True # don't change Hugh Laurie's last name just b.c. it could be a female first name\n",
        "            if 'female' in conventional_binary_gen:\n",
        "              alt_word = random.choice(recc_names['M'])\n",
        "            elif 'male' in conventional_binary_gen:\n",
        "              alt_word = random.choice(recc_names['F'])\n",
        "            # leave ambiguous or unknown names alone\n",
        "\n",
        "        else:\n",
        "            just_saw_proper_noun = False\n",
        "\n",
        "            if token.pos_ == \"NOUN\" or token.pos_ == \"PRON\":\n",
        "              if 'AdvType=Tim' not in token.tag_: # don't change years\n",
        "                alt_word = pub_flip_noun(token)\n",
        "            \n",
        "            elif len(pairings) > 0 and token.text == pairings[0][0].text:\n",
        "                diff = noun_diff\n",
        "                if token.text.lower() in ['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'estas', 'estes', 'estos', 'aquello', 'aquella', 'aquellos', 'aquellas']:\n",
        "                  diff = article_diff\n",
        "                \n",
        "                dep_noun_token = pairings[0][1]\n",
        "                if (('PROPN_' not in dep_noun_token.tag_) and ('Gender' in token.tag_)):\n",
        "                  alt_noun = pub_flip_noun(dep_noun_token)\n",
        "                  if alt_noun is not None: # don't change ADJ if the noun would not change\n",
        "                    # print(token.text + \" ? \" + str(alt_noun))\n",
        "                    alt_noun_nlp = nlp(alt_noun)\n",
        "                    if 'Gender=Masc' in token.tag_:\n",
        "                      alt_word = closest_word(token.text, -0.6 * diff, printme=False)\n",
        "                    else:\n",
        "                      alt_word = closest_word(token.text, 0.6 * diff, printme=False)\n",
        "                pairings = pairings[1:]\n",
        "        if alt_word is None or alt_word.lower() == token.text.lower():\n",
        "            words.append(token.text)\n",
        "        else:\n",
        "            words.append(alt_word)\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWmGoxauh-yZ",
        "colab_type": "code",
        "outputId": "b2d60f3d-4268-44da-9651-2bc5a916f920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "pub_flip(\"Durante las audiciones, el actor británico Hugh Laurie se encontraba en las filmaciones de la película El vuelo del Fénix en Namibia.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Durante las audiciones , la actriz británica Paula Laurie se encontraba en las filmaciones de la película El vuelo del Fénix en Namibia .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mJpM3XylZ1I",
        "colab_type": "text"
      },
      "source": [
        "Still not great for some place names.... I thought about not resetting after a token.pos_ == \"PUNCT\", but then I realized it would disable flipping on (much more common) lists of first names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8OC43XAlR9i",
        "colab_type": "code",
        "outputId": "df6bf370-1ef3-4dd9-8672-2a68b1c6a92a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pub_flip(\"Ellos viven en Savannah, Georgia\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ellas viven en Eugenio , Ernesto'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n726xLh24Jyg",
        "colab_type": "code",
        "outputId": "e253b276-4957-4a97-f369-5580df57208f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "for token in nlp(\"Savannah, Georgia\"):\n",
        "  print(token.text + \": \" + token.tag_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Savannah: PROPN___\n",
            ",: PUNCT__PunctType=Comm\n",
            "Georgia: PROPN___\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxa_O_kpl6vr",
        "colab_type": "code",
        "outputId": "be33aef9-1cf2-4d44-b125-bb9ee3c08e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pub_flip(\"¿Dónde están Justo, Juan, y Alicia?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'¿ Dónde están Sara , Purificación , y Mario ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAM_8PZT1C1k",
        "colab_type": "text"
      },
      "source": [
        "Debugging el/la lingüista issue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF8LfZhDzr9n",
        "colab_type": "code",
        "outputId": "513fe082-b8c6-44f9-ca38-3ec532830179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for token in nlp(\"el lingüista\"):\n",
        "  print(token.text + \": \"  + token.tag_)\n",
        "\n",
        "for token in nlp(\"la biblioteca\"):\n",
        "  print(token.text + \": \"  + token.tag_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "el: DET__Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
            "lingüista: NOUN__Number=Sing\n",
            "la: DET__Definite=Def|Gender=Fem|Number=Sing|PronType=Art\n",
            "biblioteca: NOUN__Gender=Fem|Number=Sing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JzT4ywa3-A0",
        "colab_type": "code",
        "outputId": "a4670ca3-22a9-4ca7-e528-2046ef5479a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pub_flip(\"el lingüista nuevo\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'la lingüista nueva'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD-7c5M94zd6",
        "colab_type": "code",
        "outputId": "34ea6cf2-e4f6-49fd-af30-a765a1c1b302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pub_flip(\"él fue a la biblioteca.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ella fue a la biblioteca .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeHp2KLX1FiX",
        "colab_type": "code",
        "outputId": "d7e204cd-6f28-4878-aa46-765c10594793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pub_flip(\"En el siglo XX el lingüista estadounidense Noam Chomsky creó la corriente conocida como generativismo.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'En la siglo XX la lingüista estadounidense Carolina Chomsky creó la corriente conocida como generativismo .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1SAjtQeocQC",
        "colab_type": "text"
      },
      "source": [
        "Also problems when mirroring with capitalized Nosotros/Nosotras... spaCy labels Nosotros as having gender, and Nosotras as... not??"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bJn4eGa2AQ4",
        "colab_type": "code",
        "outputId": "781eddcd-288e-4de7-c21e-c93681d6a164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for token in nlp(\"Nosotras corrimos en el parque\"):\n",
        "  print(token.text + \": \" + token.tag_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nosotras: PRON__Number=Sing|Person=1\n",
            "corrimos: VERB__Mood=Cnd|Number=Plur|Person=1|VerbForm=Fin\n",
            "en: ADP__AdpType=Prep\n",
            "el: DET__Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
            "parque: NOUN__Gender=Masc|Number=Sing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0Fx95Pd5V2E",
        "colab_type": "code",
        "outputId": "0a41a594-728d-4d2a-869b-fa850ced69ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "closest_word(\"Nosotras\", 1 * noun_diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5687854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nos'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwy9oOhimnPW",
        "colab_type": "code",
        "outputId": "cb0b0e3a-0205-42bd-c090-ab47bed325b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(pub_flip(\"Nosotros corrimos en el parque\"))\n",
        "pub_flip(\"Nosotras corrimos en el parque\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nosotras corrimos en el parque\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nosotros corrimos en el parque'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMwKQUIlwm9z",
        "colab_type": "text"
      },
      "source": [
        "# Could counterfactuals flip to -@s endings?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjBybHNJyMTy",
        "colab_type": "text"
      },
      "source": [
        "The closest word to \"amig@s\" known to the language model is \"Ami\", but only because the beginning was separated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r-ns409vmbT",
        "colab_type": "code",
        "outputId": "e1936fc9-3a45-4b37-bdb4-2deadec813ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "closest_word(\"amig@s\", 0 * noun_diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.49993205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ami'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbluwevrw7GY",
        "colab_type": "code",
        "outputId": "1015d27f-e2cd-4179-da47-054f61f20b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encodeAt = tokenizer.encode(\"amig@s\")\n",
        "print(encodeAt)\n",
        "print(tokenizer.decode(encodeAt))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 1822, 30948, 3, 1020, 5]\n",
            "[CLS] amig [UNK] s [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_m7-zkLylLs",
        "colab_type": "text"
      },
      "source": [
        "A word ending with \"-e\" or \"-es\" is also not recognized in the language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2gatwqGyUJA",
        "colab_type": "code",
        "outputId": "f95906e3-ae5a-42bb-f4f2-6e13524b0d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encodeAt = tokenizer.encode(\"maestre\")\n",
        "print(encodeAt)\n",
        "print(tokenizer.decode(encodeAt))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 8062, 1297, 5]\n",
            "[CLS] maestre [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My7Y9ugnyrB1",
        "colab_type": "code",
        "outputId": "0244afc7-2bd0-4099-9b6c-7cb1005f8754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encodeAt = tokenizer.encode(\"soldades\")\n",
        "print(encodeAt)\n",
        "print(tokenizer.decode(encodeAt))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 1505, 1356, 5]\n",
            "[CLS] soldades [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}